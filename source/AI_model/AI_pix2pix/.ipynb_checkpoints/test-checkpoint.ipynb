{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BhJjrDThK6Zl"
   },
   "source": [
    "# **1. Basics**\n",
    "**Prerequisites**\n",
    "\n",
    "`pytorch`, `torchvision`, `numpy`, `openCV2`,`matplotlib`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1659,
     "status": "ok",
     "timestamp": 1573317495244,
     "user": {
      "displayName": "친절배려양보",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDFkwnEU7dwPLhibVEPf81KRoxg6Fp6WyoycQ8S_w=s64",
      "userId": "13918714700772710322"
     },
     "user_tz": -540
    },
    "id": "I3KEZd2bd1Pm",
    "outputId": "ec9a9597-26e1-43aa-aa11-af810d9725fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "The gpu to be used : NVIDIA GeForce GTX 1660 Ti\n"
     ]
    }
   ],
   "source": [
    "# For plotting\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# For utilities\n",
    "import time, sys, os\n",
    "\n",
    "# For conversion\n",
    "import cv2\n",
    "import opencv_transforms.transforms as TF\n",
    "import dataloader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# For everything\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "# For our model\n",
    "import mymodels\n",
    "import torchvision.models\n",
    "\n",
    "# To ignore warning\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "if device=='cuda':\n",
    "    print(\"The gpu to be used : {}\".format(torch.cuda.get_device_name(0)))\n",
    "else:\n",
    "    print(\"No gpu detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3_cw23W9d1P5"
   },
   "source": [
    "# **2. Loading dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint     im_test_edge_2.jpg  mymodels.py\tREADME.md\r\n",
      "dataloader.py  im_test_edge.jpg    outputs\ttest.ipynb\r\n",
      "dataset        Model_details.pdf   pixtopix.py\ttest_opencv.ipynb\r\n",
      "encode.png     Model.png\t   __pycache__\ttrain.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WfPjBNrcAXLw"
   },
   "source": [
    "## 2.1 Color to sketch converter\n",
    "\n",
    "The `netC2S` is a network that convert a colorful image to sketch image. The network was pretrained on the dataset [1]. Even though there are some other methods to convert image to sketch like edge detection, Just forwding another pretrained network gives better result.\n",
    "\n",
    "To load the model weights, download the checkpoint on https://drive.google.com/open?id=1pIZCjubtyOUr7AXtGQMvzcbKczJ9CtQG (449MB) and unzip on directory `./checkpoint`. \n",
    "Then the file on `./checkpoint/color2edge/ckpt.pth` will be loaded.\n",
    "\n",
    "[1] Taebum Kim, \"Anime Sketch Colorization Pair\", https://www.kaggle.com/ktaebum/anime-sketch-colorization-pair, 2019., 2020.1.13."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained Color2Sketch model... Done!\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    netC2S = mymodels.Color2Sketch(pretrained=True).to(device)\n",
    "    netC2S.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W0wM8z9abn-P"
   },
   "source": [
    "## 2.2 Load data\n",
    "\n",
    "To pre-process input images, the module `opencv_transforms.transforms` and `opencv_transforms.functional` are used. These are implemented with **openCV** so much faster than `torchvision.transforms` which is based on **Pillow**.[2] You can download the module on [2].\n",
    "\n",
    "To download validation dataset, go [1] and download. Unzip the images on directory `./dataset/val/`.\n",
    "\n",
    "I've alreay set some validation and test images. If you want test on orther images, put the image on `./dataset/test/`. \n",
    "\n",
    "[2] Jim Bohnslav,\"opencv_transforms\", https://github.com/jbohnslav/opencv_transforms, 2020.1.13."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1035,
     "status": "ok",
     "timestamp": 1573317630904,
     "user": {
      "displayName": "친절배려양보",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDFkwnEU7dwPLhibVEPf81KRoxg6Fp6WyoycQ8S_w=s64",
      "userId": "13918714700772710322"
     },
     "user_tz": -540
    },
    "id": "mQe2xM2sAbVy",
    "outputId": "dc0237b9-6c86-430e-d239-214be395419b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Validation data... Done!\n",
      "Validation data size : 3545\n",
      "Loading Test data... Done!\n",
      "size: Dataset GetImageFolder\n",
      "    Number of datapoints: 1\n",
      "    Root location: ./dataset/test_void\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=512, interpolation=bilinear)\n",
      "           )\n",
      "Loading Reference data... Done!\n",
      "Reference data size : 1\n"
     ]
    }
   ],
   "source": [
    "# batch_size. number of cluster\n",
    "batch_size = 1\n",
    "ncluster = 9\n",
    "\n",
    "# Validation \n",
    "print('Loading Validation data...', end=' ')\n",
    "val_transforms = TF.Compose([\n",
    "    TF.Resize(512),\n",
    "    ])\n",
    "val_imagefolder = dataloader.PairImageFolder('./dataset/val_void', val_transforms, netC2S, ncluster)\n",
    "val_loader = torch.utils.data.DataLoader(val_imagefolder, batch_size=batch_size, shuffle=False)\n",
    "print(\"Done!\")\n",
    "print(\"Validation data size : {}\".format(len(val_imagefolder)))\n",
    "\n",
    "\n",
    "# Test\n",
    "print('Loading Test data...', end=' ')\n",
    "test_transforms = TF.Compose([\n",
    "    TF.Resize(512),\n",
    "    ])\n",
    "test_imagefolder = dataloader.GetImageFolder('./dataset/test_void', test_transforms, netC2S, ncluster)\n",
    "test_loader = torch.utils.data.DataLoader(test_imagefolder, batch_size=batch_size, shuffle=False)\n",
    "print(\"Done!\")\n",
    "\n",
    "print(\"size:\",format(test_imagefolder))\n",
    "# Reference\n",
    "print('Loading Reference data...', end=' ')\n",
    "refer_transforms = TF.Compose([\n",
    "    TF.Resize(512),\n",
    "    ])\n",
    "refer_imagefolder = dataloader.GetImageFolder('./dataset/reference_void', refer_transforms, netC2S, ncluster)\n",
    "refer_loader = torch.utils.data.DataLoader(refer_imagefolder, batch_size=1, shuffle=False)\n",
    "refer_batch = next(iter(refer_loader))\n",
    "print(\"Done!\")\n",
    "print(\"Reference data size : {}\".format(len(refer_imagefolder)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DcBgaktjD3Ul"
   },
   "source": [
    "## 2.3 Dataset Test\n",
    "\n",
    "Check the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_batch_iter = iter(refer_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 216
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1797,
     "status": "ok",
     "timestamp": 1573317635511,
     "user": {
      "displayName": "친절배려양보",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDFkwnEU7dwPLhibVEPf81KRoxg6Fp6WyoycQ8S_w=s64",
      "userId": "13918714700772710322"
     },
     "user_tz": -540
    },
    "id": "90y9V_zqeb93",
    "outputId": "aad3aaf3-ffcc-48ed-ccca-da0c48adc607",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "temp_batch = next(temp_batch_iter)\n",
    "edge = temp_batch[0]\n",
    "\n",
    "\n",
    "# Convertir le tenseur en une forme appropriée\n",
    "temp_tensor = edge.squeeze().permute(0,1,2)\n",
    "\n",
    "color = temp_batch[1]\n",
    "color_palette = temp_batch[2]\n",
    "\n",
    "color_list = temp_batch[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cdYb-1jfE8Lh"
   },
   "source": [
    "# **3. Load the Model**\n",
    "\n",
    "The model are implemented on `mymodels.py`.\n",
    "\n",
    "To load the model weights, download the checkpoint on https://drive.google.com/open?id=1pIZCjubtyOUr7AXtGQMvzcbKczJ9CtQG (449MB) and unzip on directory `./checkpoint`. \n",
    "Then the file on `./checkpoint/edge2color/ckpt.pth` will be loaded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n",
      "Loading pretrained Sketch2Color model... Done!\n",
      "Number of parameters: 63504297\n"
     ]
    }
   ],
   "source": [
    "# A : Edge, B : Color\n",
    "nc = 3 * (ncluster + 1)\n",
    "print(type(nc))\n",
    "netG = mymodels.Sketch2Color(nc=nc, pretrained=True).to(device) \n",
    "\n",
    "num_params = sum(p.numel() for p in netG.parameters() if p.requires_grad)\n",
    "print('Number of parameters: %d' % (num_params))\n",
    "\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tFSZEPYJd1Q7"
   },
   "source": [
    "# **4. Results**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Show colorization results\n",
    "\n",
    "Show colorization results on val/test batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_batch_iter=iter(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: Invalid image width in IHDR\n",
      "libpng warning: Image width exceeds user limit in IHDR\n",
      "libpng warning: Invalid image height in IHDR\n",
      "libpng warning: Image height exceeds user limit in IHDR\n",
      "libpng error: Invalid IHDR data\n",
      "/home/labougit/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:645: FutureWarning: `cached_download` is the legacy way to download files from the HF hub, please consider upgrading to `hf_hub_download`\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://huggingface.co/eugenesiow/edsr/resolve/main/pytorch_model_2x.pt\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "netG.eval()\n",
    "temp_batch = next(temp_batch_iter)\n",
    "\n",
    "from PIL import Image\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "# Charger l'image avec PIL\n",
    "img = temp_batch[0].squeeze()\n",
    "\n",
    "# Convertir l'image en un tableau NumPy\n",
    "img_np = np.array(img)\n",
    "\n",
    "# Définir les transformations à appliquer à l'image\n",
    "transform = TF.Compose([\n",
    "    TF.Resize((512,512)),\n",
    "    TF.ToTensor(),             # convertir l'image en un tensor PyTorch\n",
    "])\n",
    "\n",
    "# Appliquer les transformations à l'image\n",
    "edge = transform(img_np)\n",
    "edge_tmp = edge.unsqueeze(0)\n",
    "# Convertir le tensor en une image PIL\n",
    "#edge_img = TF.ToPILImage(edge_tmp.squeeze())\n",
    "\n",
    "# Afficher l'image avec matplotlib\n",
    "#plt.imshow(edge_tmp)\n",
    "#plt.show()\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    #notre edge\n",
    "    edge = edge_tmp\n",
    "    edge = edge.to(device)  # Déplacez edge sur le GPU\n",
    "    \n",
    "    encode = netC2S.forward(edge)\n",
    "    \n",
    "    #affichage de l'image après l'autoencoder\n",
    "    encode = encode.permute(0, 2, 3, 1)\n",
    "\n",
    "    # Afficher l'image à l'aide de Matplotlib des on encoder\n",
    "    encode = encode.detach().cpu().numpy()\n",
    "    img = (encode[0] * 255).astype('uint8')\n",
    "    cv2.imwrite('./outputs/sketch.png', img)\n",
    "    #edge de base\n",
    "    #edge = temp_batch[0].to(device)\n",
    "\n",
    "    real = temp_batch[1].to(device)\n",
    "    \n",
    "    reference = refer_batch[1].to(device)\n",
    "    \n",
    "    \n",
    "    color_palette = refer_batch[2]\n",
    "\n",
    "        \n",
    "\n",
    "    input_tensor = torch.cat([edge.cpu()]+color_palette, dim=1).to(device)\n",
    "    fake = netG(input_tensor)\n",
    "    result = torch.cat((reference, edge, fake), dim=-1).cpu()\n",
    "    \n",
    "    #result_for_save = torch.cat(fake, dim=-1)\n",
    "    output_for_save = vutils.make_grid(fake, nrow=1, padding=5, normalize=True).cpu().permute(1,2,0).numpy()\n",
    "            \n",
    "    # Save images to file\n",
    "    save_path = 'outputs/'\n",
    "    save_name = 'img-{}.jpg'.format(\"image_genere\")\n",
    "    plt.imsave(arr=output_for_save, fname='{}{}'.format(save_path, save_name))\n",
    "            \n",
    "    # Convertir le tensor en une image PIL\n",
    "    to_pil = transforms.ToPILImage()\n",
    "    pil_image = to_pil(fake[0])\n",
    "\n",
    "    folder_path = './dataset/test_void/test'\n",
    "\n",
    "    # Obtenez la liste des fichiers dans le dossier\n",
    "    files = os.listdir(folder_path)\n",
    "\n",
    "    # Bouclez à travers tous les fichiers dans le dossier\n",
    "    for file in files:\n",
    "    # Vérifiez si le fichier est une image en fonction de son extension\n",
    "        if file.endswith('.jpg') or file.endswith('.jpeg') or file.endswith('.png'):\n",
    "        # Ouvrez l'image en utilisant la bibliothèque Pillow\n",
    "            image = Image.open(os.path.join(folder_path, file))\n",
    "            width_ori,height_ori = image.size\n",
    "    #Upscale Image\n",
    "    from super_image import EdsrModel, ImageLoader\n",
    "    from PIL import Image\n",
    "    import requests\n",
    "    \n",
    "    model = EdsrModel.from_pretrained('eugenesiow/edsr', scale=2)   \n",
    "    img = Image.open(\"outputs/img-image_genere.jpg\")\n",
    "    \n",
    "    inputs = ImageLoader.load_image(img)\n",
    "    preds = model(inputs)\n",
    "\n",
    "    ImageLoader.save_image(preds, './outputs/scaled_2x.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 REDIMENSIONNEMENT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upscale Image\n",
    "from super_image import EdsrModel, ImageLoader\n",
    "from PIL import Image\n",
    "import requests\n",
    "    \n",
    "model = EdsrModel.from_pretrained('eugenesiow/edsr', scale=2)   \n",
    "img = Image.open(\"outputs/img-image_genere.jpg\")\n",
    "    \n",
    "inputs = ImageLoader.load_image(img)\n",
    "preds = model(inputs)\n",
    "\n",
    "ImageLoader.save_image(preds, './outputs/scaled_2x.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger l'image\n",
    "image = Image.open(\"outputs/scaled_2x.png\")\n",
    "new_size = (width_ori, height_ori)\n",
    "image_resized = image.resize(new_size)\n",
    "# Enregistrer l'image dans un fichier\n",
    "image_resized.show()\n",
    "image_resized.save('./outputs/image_final_redimensionné.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "main.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
